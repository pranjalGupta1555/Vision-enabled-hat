This project is a single joint architecture on a hat working as both the visual understanding of the input images and a language model to generate meaningful semantically correct captions for the images along with providing an optimal direction for the blind to navigate in a crowded area. 
It involves both image and language processing using CNN and RNN respectively.
